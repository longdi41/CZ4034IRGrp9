{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66dd5370-ad17-4823-a9cf-db2e8bc4236e",
   "metadata": {},
   "source": [
    "transformer: https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb#scrollTo=YehEMfsU3Brr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008f42a-04b3-4154-a693-54aaf975276e",
   "metadata": {},
   "source": [
    "lstm: https://www.analyticsvidhya.com/blog/2021/06/natural-language-processing-sentiment-analysis-using-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afd1f02-357d-4146-9252-acb5337117e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import neattext.functions as nf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61142be0-b964-4e06-b0a2-7b1e9424f79c",
   "metadata": {},
   "source": [
    "## 1. To classify the attitude that Russia will win/lose the war (Enhanced Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63535bc6-a05e-4f0c-8c14-501aa27dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('will_russia_win/no.txt','r') as f:\n",
    "    s = f.read().split('\\n')\n",
    "    s = [ss for ss in s if ss!='']\n",
    "    s = [nltk.tokenize.sent_tokenize(ss) for ss in s]\n",
    "    no = [item for sublist in s for item in sublist]\n",
    "with open('will_russia_win/yes.txt','r') as f:\n",
    "    s = f.read().split('\\n')\n",
    "    s = [ss for ss in s if ss!='']\n",
    "    s = [nltk.tokenize.sent_tokenize(ss) for ss in s]\n",
    "    yes = [item for sublist in s for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35cdf2de-f849-42cd-a8b1-a9a05fe4316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tags = r\"@\\w*\"\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'@russia','russia ',text)\n",
    "    text = re.sub(r'@Russia','russia ',text)\n",
    "    text = re.sub(r'@ukraine','ukraine ',text)\n",
    "    text = re.sub(r'@Ukraine','ukraine ',text)\n",
    "    text = re.sub(r'@nato','nato ',text)\n",
    "    text = re.sub(r'@Nato','nato ',text)  \n",
    "    text = re.sub(r'@NATO','nato ',text)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+','',text)\n",
    "    text = re.sub(r'/',' ',text)\n",
    "    text = re.sub(r'\\n',' ',text)\n",
    "    return text\n",
    "\n",
    "def clean_word(words):\n",
    "    lancaster=LancasterStemmer()\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        if not word in stopwords.words('english'):\n",
    "            if not bool(re.search(r'\\d', word)):\n",
    "                word_list.append(lancaster.stem(word))\n",
    "    return word_list\n",
    "\n",
    "def list_to_str(l):\n",
    "    res = ''\n",
    "    for s in l:\n",
    "        res = res +' '+s\n",
    "    return res.strip()\n",
    "def processList(yes):\n",
    "    yes = [cleanText(x) for x in yes]\n",
    "    yes=pd.Series(yes)\n",
    "    yes = yes.apply(nf.remove_urls)\n",
    "    yes = yes.apply(nf.remove_userhandles)\n",
    "    yes = yes.apply(nf.remove_hashtags)\n",
    "    yes = yes.apply(nf.remove_puncts)\n",
    "    yes = yes.apply(nf.remove_emojis)\n",
    "    yes = yes.apply(nf.remove_special_characters)\n",
    "    yes = yes.apply(nf.remove_multiple_spaces)\n",
    "    yes = yes.str.lower()\n",
    "    yes = [word_tokenize(i) for i in yes]\n",
    "    yes = [clean_word(i) for i in yes]\n",
    "    yes = [y for y in yes if len(y)>2]\n",
    "    yes = [list_to_str(i) for i in yes]\n",
    "    return yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a7350af-2adf-4917-8bf8-778875cef399",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = processList(yes)\n",
    "no = processList(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc417e0b-2f82-48cd-b493-f6a41b16cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_label = [1]*len(yes)\n",
    "no_label = [0]*len(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b79ab35-24bb-40b5-be33-c997a9c0ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = yes+no\n",
    "label = yes_label+no_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e04f73c-9a92-4316-912d-57c4f794a47d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/qnh6g1s13mzf5hykcng3_3z40000gn/T/ipykernel_42082/2397496247.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext_tf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m114514\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(text)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tf, label, test_size=0.2, random_state=114514)\n",
    "X_train, X_test = X_train.toarray(), X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "298751c3-8575-407a-9597-dc747b566957",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf-pkl/attitude.pkl','wb') as f:\n",
    "    pickle.dump(tf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76afe89-6172-4367-a649-58be051c9031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {\n",
    "    'n_neighbors':list(range(1,20,2))+list(range(20,105,5)),\n",
    "    'weights':['uniform', 'distance']\n",
    "}\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression()\n",
    "logistic_params = {\n",
    "    'penalty':['l1','l2','elasticnet','none']\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_params = {\n",
    "    'n_estimators':list(range(2,21,2)),\n",
    "    'criterion':['gini','entropy'],\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc_params = {\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'splitter':['best','random']\n",
    "}\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "rbf_svc = SVC()\n",
    "rbf_svc_params = {\n",
    "    'C':[0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "lin_svc = SVC()\n",
    "lin_svc_params = {\n",
    "    'kernel':['linear'],\n",
    "    'C':[0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb_params = {\n",
    "    'alpha':[0,0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_params = {\n",
    "    'loss':['deviance','exponential'],\n",
    "    'learning_rate':[0.0001,0.001,0.01,0.1],\n",
    "    'n_estimators':[2,10,50,100,200],\n",
    "    'criterion':['friedman_mse','squared_error','mse','mae']\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.ensemble import  AdaBoostClassifier\n",
    "abc = AdaBoostClassifier()\n",
    "abc_params={\n",
    "    'learning_rate':[0.0001,0.001,0.01,0.1],\n",
    "    'n_estimators':[2,10,50,100,200],\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_params = {\n",
    "    'solver':['svd','lsqr','eigen']\n",
    "}\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda_params = {\n",
    "    'reg_param':[0,0.001,0.001,0.01,0.1]\n",
    "}\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier()\n",
    "nn_params = {\n",
    "    'alpha':[0.01,0.1,1],\n",
    "    'max_iter':[2000]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    'knn':knn,\n",
    "    'logistic_regression':logistic,\n",
    "    'random_forest':rfc,\n",
    "    'decision_tree':dtc,\n",
    "    'rbf_svc':rbf_svc,\n",
    "    'linear_svc':lin_svc,\n",
    "    'naive_bayes':mnb,\n",
    "    'adaboost':abc,\n",
    "    'linear_discriminant':lda,\n",
    "    'quadratic_discriminant':qda,\n",
    "    'neural_network':nn\n",
    "    \n",
    "    \n",
    "}\n",
    "params = {\n",
    "    'knn':knn_params,\n",
    "    'logistic_regression':logistic_params,\n",
    "    'random_forest':rfc_params,\n",
    "    'decision_tree':dtc_params,\n",
    "    'rbf_svc':rbf_svc_params,\n",
    "    'linear_svc':lin_svc_params,\n",
    "    'naive_bayes':mnb_params,\n",
    "    'adaboost':abc_params,\n",
    "    'linear_discriminant':lda_params,\n",
    "    'quadratic_discriminant':qda_params,\n",
    "    'neural_network':nn_params\n",
    "}\n",
    "\n",
    "best={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6c6a8c-4bc3-4443-83d6-f6c19d00d2db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de924c8ea8b7494d9600477e3fd30110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/qnh6g1s13mzf5hykcng3_3z40000gn/T/ipykernel_30263/1839508640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "for k in tqdm(classifiers):\n",
    "    gcv = GridSearchCV(estimator=classifiers[k], param_grid=params[k], n_jobs=-1)\n",
    "    gcv.fit(X_train, y_train)\n",
    "    best[k]=gcv.best_estimator_\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce834cdb-d883-4066-b846-8fefa8182604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abebe031fcc4ed79c0a015c9bf97242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score_dict = {}\n",
    "for k in tqdm(best):\n",
    "    with open(f'sklearn-pkl/attitude/{k}.pkl','wb') as f:\n",
    "        pickle.dump(best[k],f)\n",
    "    y_pred = best[k].predict(X_test)\n",
    "    score_dict[k]={\n",
    "        'accuracy':accuracy_score(y_pred,y_test),\n",
    "        'precision':precision_score(y_pred,y_test),\n",
    "        'recall':recall_score(y_pred,y_test),\n",
    "        'f1':f1_score(y_pred,y_test),\n",
    "        'Details':str(best[k])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1282edc7-e79d-4b81-8b73-b94311c1c015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression</th>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.5</td>\n",
       "      <td>LogisticRegression(penalty='none')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>RandomForestClassifier(n_estimators=14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy', sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rbf_svc</th>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>SVC(C=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear_svc</th>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>SVC(C=1, kernel='linear')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_bayes</th>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>MultinomialNB(alpha=0.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaboost</th>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AdaBoostClassifier(learning_rate=0.0001, n_est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear_discriminant</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>LinearDiscriminantAnalysis()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quadratic_discriminant</th>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(reg_param=0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural_network</th>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>MLPClassifier(alpha=0.01, max_iter=2000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accuracy precision    recall        f1  \\\n",
       "knn                     0.703704  0.423077      0.55  0.478261   \n",
       "logistic_regression     0.753086  0.384615  0.714286       0.5   \n",
       "random_forest           0.703704  0.076923       1.0  0.142857   \n",
       "decision_tree           0.728395  0.461538       0.6  0.521739   \n",
       "rbf_svc                 0.679012  0.076923       0.5  0.133333   \n",
       "linear_svc              0.753086  0.269231     0.875  0.411765   \n",
       "naive_bayes             0.679012  0.384615       0.5  0.434783   \n",
       "adaboost                0.679012       0.0       0.0       0.0   \n",
       "linear_discriminant     0.703704  0.423077      0.55  0.478261   \n",
       "quadratic_discriminant  0.679012       0.0       0.0       0.0   \n",
       "neural_network          0.716049  0.423077  0.578947  0.488889   \n",
       "\n",
       "                                                                  Details  \n",
       "knn                                   KNeighborsClassifier(n_neighbors=1)  \n",
       "logistic_regression                    LogisticRegression(penalty='none')  \n",
       "random_forest                     RandomForestClassifier(n_estimators=14)  \n",
       "decision_tree           DecisionTreeClassifier(criterion='entropy', sp...  \n",
       "rbf_svc                                                         SVC(C=10)  \n",
       "linear_svc                                      SVC(C=1, kernel='linear')  \n",
       "naive_bayes                                      MultinomialNB(alpha=0.1)  \n",
       "adaboost                AdaBoostClassifier(learning_rate=0.0001, n_est...  \n",
       "linear_discriminant                          LinearDiscriminantAnalysis()  \n",
       "quadratic_discriminant     QuadraticDiscriminantAnalysis(reg_param=0.001)  \n",
       "neural_network                   MLPClassifier(alpha=0.01, max_iter=2000)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attitude_result = pd.DataFrame(score_dict).T\n",
    "attitude_result.to_csv('attitude_result.csv')\n",
    "attitude_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561c93f-a2ee-45ee-a470-d00709c49483",
   "metadata": {},
   "source": [
    "## 2. Spatial Classification (Enhanced Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944ba1a1-97b8-436f-b18e-4e79553517d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data_with_city.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a39cf8b-6c3a-42be-9d12-1755d2423f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/zhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zhang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada58557035d4f2dbea4aa12ef25e760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0566748eb43a494bbaa8ecc589b177b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b8bd884b724083b849e975f50e9a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "def clean(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    return text\n",
    "df1['cleaned'] = df1['Text'].apply(clean)\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# POS tagger dictionary\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "df1['tagged'] = df1['cleaned'].progress_apply(token_stop_pos)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "df1['lemma'] = df1['tagged'].progress_apply(lemmatize)\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "def stemmize(tagged):\n",
    "    stemmed = ''\n",
    "    for word, _ in tagged:\n",
    "        stemmed = stemmed +' '+lancaster.stem(word)\n",
    "    return stemmed.strip()\n",
    "\n",
    "df1['stemmed'] = df1['tagged'].progress_apply(stemmize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444c2087-8c5b-4bee-8460-456d3602f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8dbd162-003e-4ab4-b8bd-6d37db29003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(df1.Text)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tf, df1.country, test_size=0.2, random_state=114514)\n",
    "X_train, X_test = X_train.toarray(), X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05c26a1-2ff9-4951-b0a1-10b8cbdf04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf-pkl/geo.pkl','wb') as f:\n",
    "    pickle.dump(tf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e17981-7c91-4d4c-8fec-1de415cc2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,:5000]\n",
    "X_test = X_test[:,:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fde3efaa-353d-4ab7-940c-71e839ef4d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "logistic = LogisticRegression()\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "rbf_svc = SVC(C=0.125)\n",
    "lin_svc = SVC(kernel='linear',C=1)\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "abc = AdaBoostClassifier()\n",
    "nn = MLPClassifier(alpha=0.1,max_iter=2000)\n",
    "classifiers = {\n",
    "    'knn':knn,\n",
    "    'logistic_regression':logistic,\n",
    "    'random_forest':rfc,\n",
    "    'decision_tree':dtc,\n",
    "    # 'rbf_svc':rbf_svc,\n",
    "    # 'linear_svc':lin_svc,\n",
    "    'naive_bayes':mnb,\n",
    "    'adaboost':abc,\n",
    "    # 'linear_discriminant':lda,\n",
    "    # 'quadratic_discriminant':qda,\n",
    "    # 'neural_network':nn\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1309347-61b2-4b27-9104-c87f000483f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7936cfe4644b1d88c27f46c98341a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression\n",
      "random_forest\n",
      "decision_tree\n",
      "naive_bayes\n",
      "adaboost\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(classifiers):\n",
    "    classifiers[k]=classifiers[k].fit(X_train,y_train)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0156d8da-3a84-4542-a535-fb5b6d222542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "del classifiers['linear_discriminant']\n",
    "del classifiers['quadratic_discriminant']\n",
    "classifiers['neural_network']=classifiers['neural_network'].fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e5036bf-fba5-4620-a273-8c83e8cb000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifiers['neural_network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32bd7e3-aebd-4876-8d6a-a6a727803d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9744, 5000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a4645b-13ff-45aa-9598-56fbed3ed041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn': KNeighborsClassifier(n_neighbors=10),\n",
       " 'logistic_regression': LogisticRegression(),\n",
       " 'random_forest': RandomForestClassifier(n_estimators=10),\n",
       " 'decision_tree': DecisionTreeClassifier(),\n",
       " 'naive_bayes': MultinomialNB(alpha=0.1),\n",
       " 'adaboost': AdaBoostClassifier()}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4ac9ef5-eed1-40ff-84a9-8acec7c45c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ed0645faa4493aa8229736ee8397b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_dict = {}\n",
    "for k in tqdm(classifiers):\n",
    "    with open(f'sklearn-pkl/geo/{k}.pkl','wb') as f:\n",
    "        pickle.dump(classifiers[k],f)\n",
    "    y_pred = classifiers[k].predict(X_test)\n",
    "    score_dict[k]={\n",
    "        'accuracy':accuracy_score(y_pred,y_test),\n",
    "        'Details':str(classifiers[k])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fec71a45-82e4-45f9-bd57-dc07129443b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.300369</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression</th>\n",
       "      <td>0.354945</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.307755</td>\n",
       "      <td>RandomForestClassifier(n_estimators=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.270825</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_bayes</th>\n",
       "      <td>0.34961</td>\n",
       "      <td>MultinomialNB(alpha=0.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaboost</th>\n",
       "      <td>0.357407</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accuracy                                  Details\n",
       "knn                  0.300369     KNeighborsClassifier(n_neighbors=10)\n",
       "logistic_regression  0.354945                     LogisticRegression()\n",
       "random_forest        0.307755  RandomForestClassifier(n_estimators=10)\n",
       "decision_tree        0.270825                 DecisionTreeClassifier()\n",
       "naive_bayes           0.34961                 MultinomialNB(alpha=0.1)\n",
       "adaboost             0.357407                     AdaBoostClassifier()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_result = pd.DataFrame(score_dict).T\n",
    "geo_result.to_csv('geo_result.csv')\n",
    "geo_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b6b9a18-46e7-4235-a03c-96f61427346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.transform(['Fuck Russians! Your Motherfucker! Fuck Putin!'])[:,:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95c2f039-9e57-4e39-ac8a-165a60f62224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GB'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers['adaboost'].predict(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766187c-9568-474c-9ab2-9619c226d453",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
